{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Setting up Debian](#debian)\n",
    "2. [Tutorial Introduction to PyMVPA](#intro)\n",
    "3. [Hands on turorial script](#handson)\n",
    "\n",
    "# Setting up Debian <a id=\"debian\"></a>\n",
    "\n",
    "PyMVPA is designed to be able to easily interface with various libraries and computing environments, these external packages only enhance functionality built into PyMVPA or add a different flavor of some algorithm. The framework has two mandatory dependencies (`SciPy`, `NiBabel`)\n",
    "\n",
    "It requires Python 2.x > 2.7 (2.6 and 2.5 might work as well), `NumPy`.\n",
    "\n",
    "The following is a list of packages that provide additional functionality leading to more efficiency when using PyMVPA\n",
    "\n",
    "1. IPython: frontend. The way to use PyMVPA interactively\n",
    "2. FSL: Preprocessing and analysis of (f)MRI data. Provides bindings to FSL output and filetypes (e.g. EV files, estimated motion correction parameters and MELODIC output directories). So for example one can use FLS’s implementation of ICA for data reduction and proceed with analyzing the estimated ICs in PyMVPA. (I installed using the FSL installation script and NeuroDebian package)\n",
    "3. AFNI: Preprocessing and analysis of (f)MRI data. AFNI has the ability to read and write NIFTI files, which easily integrate with PyMVPA (Recommended)\n",
    "4. scikit-learn: large parts of its functionality. PyMVPA can make use of any algorithm that implements the transformer or estimator and predictor API. (Recommended)\n",
    "5. Shogun: various classifiers. Use of several SVM implementations, version from 0.6 (Recommended)\n",
    "6. LIBSVM: fast SVM classifier.  (Recommended)\n",
    "7. R and RPy: more classifiers. Currently PyMVPA provides wrappers around LARS, ElasticNet, and GLMNet R libraries available from CRAN (Recommended)\n",
    "8. matplotlib: Matlab-style plotting library for Python. Allows you to export into a large variety of raster and vector formats (e.g. SVG) and thus produce publication quality figures (Recommended)\n",
    "\n",
    "AT THIS POINT OF THE DOCUMENTATION I INSTALLED DEBIAN ON MY DESKTOP COMPUTER. I HAVE A BOOTABLE CD AND A USB. JUST FOLLOWED THE INSTRUCTIONS IN THIS [WEBSITE](https://pchelp.ricmedia.com/how-to-install-debian-linux/)\n",
    "\n",
    "AND FOR INSTALLING THE COMMAND sudo PERFORM THE STEPS IN THIS [WEBSITE](https://www.ihaveapc.com/2018/11/how-to-fix-sudo-command-not-found-in-debian-after-a-new-install/)\n",
    "\n",
    "FINALLY RAN THE FOLLOWING SCRIPT IN TERMINAL FOR [INSTALLING NEURODEBIAN](http://neuro.debian.net/install_pkg.html?p=cde)\n",
    "\n",
    "Within a terminal make sure you have installed python by writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install python2.7\n",
    "sudo apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this error, you need to add the necessary Debian software repositories in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano /etc/apt/sources.list\n",
    "\n",
    "deb http://deb.debian.org/debian stretch main\n",
    "deb-src http://deb.debian.org/debian stretch main\n",
    "# Save the file and\n",
    "sudo apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I modified the source.list file by also including the Kali repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install python-numpy\n",
    "sudo apt-get install python-mvpa2\n",
    "sudo apt-get install python-scipy\n",
    "sudo apt-get install python-matplotlib ipython python\n",
    "sudo apt-get install python-pandas python-sympy python-nose \n",
    "sudo apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I installed FSL which is helpful for doing pre processing, I download the python script, from the FSL website and run the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Downloads\n",
    "python fslinstaller.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards I found that a faster and more convenient way of installing packages was through the pip in python, so I installed for my system with the following command and verified the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install python-pip\n",
    "pip --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following tried to install packages with `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While installing this library I found a website with plenty of [resources on ML](https://scikit-learn.org/stable/auto_examples/index.html), check it out\n",
    "\n",
    "In order to install shogun package for machine learning and some classification functionalities I had to compile it manually, thus first I downloaded it from their git-hub repository, installed the cmaker package and followed the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install cmake\n",
    "sudo apt-get install git\n",
    "sudo apt-get install quilt\n",
    "sudo apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I tried several lines but **failed to compiled manually**, learned a couple of things but didn’t achieve the ultimate result. Anyways continuing with the rest of the packages. The other packages included R, so I installed R-studio, downloaded  a .deb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su\n",
    "apt-get update\n",
    "sudo apt install ~/Downloads/rstudio.deb\n",
    "\n",
    "pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I installed matplotlib with the script above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Introduction to PyMVPA <a id=\"intro\"></a>\n",
    "\n",
    "The original tutorial can be found in this [link](http://www.pymvpa.org/tutorial.html). This tutorial includes an introduction to the most important concepts, functionalities and analysis examples.\n",
    "\n",
    "The tutorial pre requisites include knowledge in Python among [others](http://www.pymvpa.org/tutorial_prerequisites.html#chap-tutorial-prerequisites)\n",
    "\n",
    "Throughout the tutorial there will be exercises with tasks that aim to deepen your understanding of a particular problem or to train important skills. Is recommended to run the tutorial code interactively and explore code snippets beyond what is touched in the tutorial. Only the most important aspects will be mentioned and each building block in PyMVPA can be used in more flexible ways than what is shown.\n",
    "\n",
    "## Recommended Reading and Viewing ##\n",
    "\n",
    "#### Tutorial Introductions Into General Python Programming ####\n",
    "_________________________________________________________\n",
    "A tutorial introducing [Python 2.6](https://en.wikibooks.org/wiki/Non-Programmer's_Tutorial_for_Python_2.6), I am doing my own summary in [here](http://localhost:8889/notebooks/Desktop/5%20CODE/3%20PYTHON/CC%20PYTHON%20PRACTICE/02%20Non-Programmers's%20Tutorial%20for%20Python%202.6.ipynb).\n",
    "Additionally another [tutorial written by the creator of Python](https://docs.python.org/3/tutorial/), this is a more comprehensive, but also more compressed tutorial that can serve as reference.\n",
    "\n",
    "#### Scientific Computing In Python ####\n",
    "___________________________________\n",
    "To employ Python for scientific computing the `Numpy` package is recommended. PyMVPA makes extensive use of `Numpy` data structures and functions, therefore we recommend you to get familiar with it ([Official website](https://www.numpy.org)).\n",
    "\n",
    "#### Interactive Python Shell ####\n",
    "_____________________________\n",
    "Check the info about [IPython](https://ipython.org) in their website. The IPython documentation extensively covers the features of IPython (Interactive environment for Python). This package make interactive use of Python more enjoyable and productive.\n",
    "\n",
    "#### Multivariate Analysis of Neuroimaging Data ####\n",
    "_____________________________________________\n",
    "\n",
    "A list of reference can be found [here](http://www.pymvpa.org/references.html#chap-references); this is a related not exhaustive list of related publications. For a quick introduction into the topic read [Pereira et. al. 2009](https://www.sciencedirect.com/science/article/pii/S1053811908012263?via%3Dihub). For the generic reference on machine learning methods we would recommend a great text book [The Elements of Statistical Learning: Data Mining, Inference and Prediction](https://link.springer.com/book/10.1007%2F978-0-387-84858-7), which is available free of charge.\n",
    "For an overview of recent advances in computational approaches for modeling and decoding os stimulus and cognitive spaces we recommend video recordings from [Neural Computation 2011 Workshop at Darthmouth College](http://haxbylab.dartmouth.edu/meetings/ncworkshop11.html)\n",
    "\n",
    "## Tutorial Data ##\n",
    "\n",
    "We will analyze real BOLD fMRI data, thus you need to download the [corresponding data from the PyMVPA website](http://www.pymvpa.org/datadb/tutorial_data.html#datadb-tutorial-data), the full dataset of this study is also available [here](http://www.pymvpa.org/datadb/haxby2001.html#datadb-haxby2001)\n",
    "\n",
    "On a NeuroDebian enabled system, the tutorial data is also available from the `python-mvpa2-tutorialdata`, and help on the tutorial can be retrieved by `pymvpa2-tutorial --help`. The `pymvpa2-tutorial` command can be invoked in a console in order to launch a tutorial session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install neurodebian\n",
    "sudo apt-get install python-mvpa2\n",
    "pymvpa2-tutorial --help\n",
    "pymvpa2-tutorial --tutorial-data-path ~/home/william/data\n",
    "pymvpa2-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the tutorial running, virtually every Python script starts with some `import` statements that load functionality provided elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.tutorial_suite import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this code runs it means everything is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data basics and concepts ##\n",
    "\n",
    "A **Dataset** is the basic data container in PyMVPA. It serves as the primary form of data storage and is also a container for results from the algorithm. Most datasets in PyMVPA are represented as a two-dimensional array, where the first axis is the **samples** axis, and the second axis represents the **features** of the samples. In the simplest case, a dataset only contains _data_ that is matrix of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[  1,  1, -1],\n",
    "        [  2,  0,  0],\n",
    "        [  3,  1,  1],\n",
    "        [  4,  0, -1]]\n",
    "ds = Dataset(data) # not running correctly bcuz might be exclusive within pymvpa\n",
    "ds.shape\n",
    "len(ds)\n",
    "ds.nfeatures\n",
    "ds.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, every row vector in the `data` matrix becomes an observation, a *sample*, and every column vector represents an individual variable, a *feature*.\n",
    "The dataset assumes that the first axis of the data is to be used to define individual samples. If the dataset is created using a one-dimensional vector it will therefore have as many samples as elements in the vector, and only one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_d = [ 0, 1, 2, 3 ]\n",
    "one_ds = Dataset(one_D)\n",
    "one_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if a dataset is created from multi-dimensional data, only its second axis represents the features.\n",
    "\n",
    "FROM THIS POINT ON I SWITCHED TO iphython AS FOR FOLLOWING THIS TUTORIAL. THE .ipynb ARE AVAILABLE IN WEBSITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on turorial script <a id=\"handson\"></a>\n",
    "\n",
    "## Dataset Basics and concepts ##\n",
    "\n",
    "PyMVPA has several helper functions to load data from specialized formats, one for fMRI data is [fmri_dataset()](http://www.pymvpa.org/generated/mvpa2.datasets.mri.fmri_dataset.html#mvpa2.datasets.mri.fmri_dataset). The shortcut for the path of the directory with the fMRI data: `tutorial_data_path`.\n",
    "\n",
    "The fMRI data is stored as a NIfTI file that has all volumes of one experiment concatenated into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_data_path = '/home/william/data'\n",
    "bold_fname = os.path.join(tutorial_data_path, 'sub001',\n",
    "                         'BOLD', 'task001_run001', 'bold.nii.gz') # just give the whole path to the bold file\n",
    "ds = fmri_dataset(bold_fname)\n",
    "len(ds)     # 121 these are the volumes in the NIfTI file\n",
    "ds.features # 163840 there are voxels in the volume\n",
    "ds.shape.   # (121, 163840)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fmri_dataset` is capable of performing data masking. We need to specify a mask image. Such a mask image is generated in any fMRI analysis pipeline --may it be a full-brain mask computed during skull-stripping, or an activation map from a functional localizer. GLM-based localizer mask of ventral temporal cortex from Haxby et al (2001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0c3c52fcb538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtutorial_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/william/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmask_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtutorial_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sub0001'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'example'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "mask_fname = os.path.join(tutorial_data_path, 'sub0001',\n",
    "                          'masks','orig','vt.nii.gz') # within mvpa concatenate with /\n",
    "ds = fmri_dataset(bold_fname, mask = mask_fname)\n",
    "len(ds)\n",
    "ds.nfeatures # 577 corresponds to the non-zero elements in the mask image\n",
    "\n",
    "ds.a  # allows to explore the attribute collection and see what kind of information do they contain\n",
    "ds.sa # check the name field = time_indices, time_coords, \n",
    "ds.fa # check the name field = voxel_indices\n",
    "ds.sa.time_indices[:5]\n",
    "ds.a.voxel_eldim #\n",
    "ds.a.voxel_dim   # voxel dimension\n",
    "\n",
    "'imghdr' in ds.a # Results in TRUE \n",
    "\n",
    "print ds.a.mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample has information about its volume index in the time series and the actual acquisition time (relative to the beginning of the file). The original voxel index (referred to as `ijk`) for each feature is available too. The dataset also contains information about the dimensionality of the input volumes, voxel size, and any other NIfTI-specific information.\n",
    "\n",
    "The dataset also carries a key additional attribute: the mapper. A mapper is an important concept in PyMVPA, and hence has its own tutorial chapter.\n",
    "\n",
    "The Dataset class `copy()` [AttrDataset.copy()](http://www.pymvpa.org/generated/mvpa2.base.dataset.AttrDataset.html#mvpa2.base.dataset.AttrDataset.copy) method can strip all unwanted attributes and leave what is absolutely neccesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = ds.copy(deep = False, sa = ['time_coords'], fa=[], a=[])\n",
    "print stripped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all attributes besides `time_coords` have been filtered out. The `deep` argument set to `False` causes the function to reuse the data from the source dataset without duplicating all data in memory.\n",
    "\n",
    "### Intermediate Storage ###\n",
    "\n",
    "One might prefer to store the preprocessed data into a file for subsequent analyses. PyMVPA offers functionality to store a large variety of objects, including datasets, into **HDF5** files. PyMVPA depends on the _h5py_ package. If it is available, any dataset can be saved to a file by calling `AttrDataset.save()` with the desired filename.\n",
    "Alternatively, one can save the same dataset with maximum gzip-compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, shutil\n",
    "tempdir = tempfile.mkdtemp()\n",
    "ds.save(os.path.join(tempdir, 'mydataset.hdf5')) # extension of the file is hdf5\n",
    "\n",
    "## with max compression\n",
    "ds.save(os.path.join(tempdir,'mydataset.gzipped.hdf5'), compression = 9)\n",
    "h5save(os.path.join(tempdir,'mydataset.gzipped.hdf5'), ds, compression = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading datasets from a file is done with `h5load`, takes a filename as an argument and returns the stored dataset. Compressed data will be handled transparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = h5load(os.path.join(tempdir, 'mydataset.hdf5'))\n",
    "np.all(ds.samples == loaded.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of dataset storage is not appropriate from long-term archival of data, as it relies on a stable software environment. For long-term storage, use other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
